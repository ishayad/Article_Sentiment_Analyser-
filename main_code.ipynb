{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5AyUosK4FFr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5-JrayL4T1a"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel('Input.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJuWlmeLVMII"
      },
      "outputs": [],
      "source": [
        "urls = []\n",
        "url_id = []\n",
        "num_of_urls = len(df['URL'])\n",
        "for i in range(num_of_urls):\n",
        "  urls.append(df['URL'][i])\n",
        "  url_id.append(df['URL_ID'][i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awV6aXwlL1gO"
      },
      "outputs": [],
      "source": [
        "# Import Required Module\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "for i in range (num_of_urls):\n",
        "  #print(url_id[i])\n",
        "  # Web URL\n",
        "  Web_url = urls[i]\n",
        "\n",
        "  # Get URL Content\n",
        "  r = requests.get(Web_url)\n",
        "\n",
        "  # Parse HTML Code\n",
        "  soup = BeautifulSoup(r.content, 'html.parser')\n",
        "\n",
        "  headings = soup.find_all(['h1','tdb-title-text'])\n",
        "\n",
        "  tags = soup.find_all('div', class_=[\"td-post-content tagdiv-type\",\"td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type\"])\n",
        "\n",
        "  file_path = str(url_id[i])+'.txt'\n",
        "  file = open(file_path, 'w')\n",
        "  for heading in headings:\n",
        "    file.write(heading.text)  # Writing in file\n",
        "  for tag in tags:\n",
        "    file.write(tag.text)\n",
        "  file.close()  # Close the file\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3Hf3V8fNRl2",
        "outputId": "90180a61-1491-42a7-b3f0-53ec0b833bab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "868jcnsib82E"
      },
      "outputs": [],
      "source": [
        "stopWord1 = open('StopWords_Auditor.txt','r')\n",
        "stopWord2 = open('StopWords_Currencies.txt','r',encoding=\"latin-1\")\n",
        "stopWord3 = open('StopWords_DatesandNumbers.txt','r')\n",
        "stopWord4 = open('StopWords_Generic.txt','r')\n",
        "stopWord5 = open('StopWords_GenericLong.txt','r')\n",
        "stopWord6 = open('StopWords_Geographic.txt','r')\n",
        "stopWord7 = open('StopWords_Names.txt','r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GwQ0Yd_6YD1"
      },
      "outputs": [],
      "source": [
        "s1 = stopWord1.readlines()\n",
        "s2 = stopWord2.readlines()\n",
        "s3 = stopWord3.readlines()\n",
        "s4 = stopWord4.readlines()\n",
        "s5 = stopWord5.readlines()\n",
        "s6 = stopWord6.readlines()\n",
        "s7 = stopWord7.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYYc2LbyEfWL"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6_ttiQTWHLq"
      },
      "outputs": [],
      "source": [
        "stopWords = []\n",
        "for s in [s1,s2,s3,s4,s5,s6,s7]:\n",
        "  string = []\n",
        "  for ch in s:\n",
        "    x =re.split('[;|,|\"|?|:| |.|\\||\\n|$|%|!|£|#|@|]',ch[:-1])\n",
        "    stopWords.append(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJhKHRVhXfAe"
      },
      "outputs": [],
      "source": [
        "stopWord = []\n",
        "for i in stopWords:\n",
        "  for j in i:\n",
        "    if len(j)!=0:\n",
        "      stopWord.append(j.upper())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIVwbs6xTU7n"
      },
      "outputs": [],
      "source": [
        "stopWord1.close()\n",
        "stopWord2.close()\n",
        "stopWord3.close()\n",
        "stopWord4.close()\n",
        "stopWord5.close()\n",
        "stopWord6.close()\n",
        "stopWord7.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXBQ8UdXaC1I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZP-fqCtWBXnA"
      },
      "outputs": [],
      "source": [
        "all_files = []\n",
        "for i in range(num_of_urls):\n",
        "  file = open(str(url_id[i])+'.txt','r')\n",
        "  f = file.read()\n",
        "  a = nltk.word_tokenize(f)\n",
        "  all_files.append(a)\n",
        "  file.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = 0\n",
        "for i in all_files:\n",
        "  s += len(i)\n",
        "print(s)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TaJsiznocuq",
        "outputId": "96f5484d-397e-4d7d-ca57-4b56f618b2ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "145501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReXxMev4JV2-"
      },
      "outputs": [],
      "source": [
        "def removingStopWords(fileList):\n",
        "  newList = []\n",
        "  for word in fileList:\n",
        "    if word.upper() not in stopWord:\n",
        "      if word not in ['.','!',',','%','-','(','’',':','|']:\n",
        "        newList.append(word)\n",
        "  return newList\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "updatedList = []\n",
        "for file in all_files:\n",
        "  updatedList.append(removingStopWords(file))\n"
      ],
      "metadata": {
        "id": "TS7wiJEtKESP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s = 0\n",
        "for i in updatedList:\n",
        "  s += len(i)\n",
        "print(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UitPbSGegHu",
        "outputId": "3c17f53c-5302-4824-c7ac-c43e09060afe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "updatedList = []\n",
        "for file in all_files:\n",
        "  updatedList.append(removingStopWords(file))\n"
      ],
      "metadata": {
        "id": "YTRV2xivaMCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def countingPositiveWords(fileList):\n",
        "  positiveCount = 0\n",
        "  for word in fileList:\n",
        "    if word.lower() in pWordsList:\n",
        "      #print('true')\n",
        "      positiveCount = positiveCount+1\n",
        "  return positiveCount"
      ],
      "metadata": {
        "id": "BpTUYAWhLcfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "pScore = []\n",
        "for file in updatedList:\n",
        "  x =countingPositiveWords(file)\n",
        "  pScore.append(x)\n",
        "  #print(url_id[count], x)\n",
        "  count = count + 1"
      ],
      "metadata": {
        "id": "haaLknJ3b3JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nFile = open('negative-words.txt','r',encoding = 'ISO-8859-1')\n",
        "nWords = nFile.readlines()\n",
        "nWordsList = []\n",
        "for n in nWords:\n",
        "  if len(n[:-1]) != 0:\n",
        "    nWordsList.append(n[:-1].lower())\n",
        "\n",
        "nFile.close()"
      ],
      "metadata": {
        "id": "dX79GGeMb6KT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def countingNegativeWords(fileList):\n",
        "  negativeCount = 0\n",
        "  for word in fileList:\n",
        "    if word.lower() in nWordsList:\n",
        "      #print('true')\n",
        "      negativeCount = negativeCount-1\n",
        "  return negativeCount*-1"
      ],
      "metadata": {
        "id": "N-9byKEduLSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "nScore = []\n",
        "for file in updatedList:\n",
        "  x =countingNegativeWords(file)\n",
        "  nScore.append(x)\n",
        "  #print(url_id[count], x)\n",
        "  count = count + 1"
      ],
      "metadata": {
        "id": "EG6mjDnZurTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def polarityScore(index):\n",
        "  return (pScore[index] - nScore[index]) / ((pScore[index] + nScore[index]) + 0.000001)\n"
      ],
      "metadata": {
        "id": "HxOzr791woNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "polScore = []\n",
        "for i in range(num_of_urls):\n",
        "  polScore.append(polarityScore(i))\n"
      ],
      "metadata": {
        "id": "f-F8D5yCxzfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def subjectiveScore(index):\n",
        "  return(pScore[index] + nScore[index])/(len(updatedList[index])+0.000001)"
      ],
      "metadata": {
        "id": "Bw55rKOZyXG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subScore = []\n",
        "for i in range(num_of_urls):\n",
        "  subScore.append(subjectiveScore(i))"
      ],
      "metadata": {
        "id": "jGCmYwFRzM7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openpyxl import load_workbook\n"
      ],
      "metadata": {
        "id": "QeyvRps10DKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workbook = load_workbook(filename='Output Data Structure.xlsx')"
      ],
      "metadata": {
        "id": "NjKdD8NM3q5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "worksheet = workbook['Sheet1']"
      ],
      "metadata": {
        "id": "-mnfraEY3-CH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_cell_coordinates(worksheet, cell_value):\n",
        "    for row in worksheet.iter_rows():\n",
        "        for cell in row:\n",
        "            if cell.value == cell_value:\n",
        "                return cell.row, cell.column\n"
      ],
      "metadata": {
        "id": "bh1QpvD04B9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cell_value = 'POSITIVE SCORE'\n",
        "row_num, column_num = find_cell_coordinates(worksheet, cell_value)\n",
        "for r in range(1,num_of_urls+1):\n",
        "  worksheet.cell(row=row_num+r, column=column_num).value = pScore[r-1]\n"
      ],
      "metadata": {
        "id": "8sMto8Ug4FW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cell_value = 'NEGATIVE SCORE'\n",
        "row_num, column_num = find_cell_coordinates(worksheet, cell_value)\n",
        "for r in range(1,num_of_urls+1):\n",
        "  worksheet.cell(row=row_num+r, column=column_num).value = nScore[r-1]"
      ],
      "metadata": {
        "id": "O2gGMTQ365PY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cell_value = 'POLARITY SCORE'\n",
        "row_num, column_num = find_cell_coordinates(worksheet, cell_value)\n",
        "for r in range(1,num_of_urls+1):\n",
        "  worksheet.cell(row=row_num+r, column=column_num).value = polScore[r-1]"
      ],
      "metadata": {
        "id": "_j_qHFxg7Amc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cell_value = 'SUBJECTIVITY SCORE'\n",
        "row_num, column_num = find_cell_coordinates(worksheet, cell_value)\n",
        "for r in range(1,num_of_urls+1):\n",
        "  worksheet.cell(row=row_num+r, column=column_num).value = subScore[r-1]"
      ],
      "metadata": {
        "id": "126bk9gQ7LQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2"
      ],
      "metadata": {
        "id": "MkpTGLmlCkIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8mYAO4xRnOK",
        "outputId": "1930e73e-f8ad-46ff-df13-ed1f932028b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "allFiles = []\n",
        "for i in range(num_of_urls):\n",
        "  file = open(str(url_id[i])+'.txt','r')\n",
        "  f = file.read()\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  stop_words.add(',')\n",
        "  stop_words.add('!')\n",
        "  stop_words.add('?')\n",
        "  stop_words.add('-')\n",
        "  stop_words.add('(')\n",
        "  stop_words.add(')')\n",
        "  stop_words.add(':')\n",
        "  stop_words.add(';')\n",
        "  stop_words.add('“')\n",
        "  stop_words.add('”')\n",
        "  stop_words.add(\"'\")\n",
        "  word_tokens = word_tokenize(f)\n",
        "  a = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "  allFiles.append(a)\n",
        "  file.close()\n"
      ],
      "metadata": {
        "id": "7JjAQeuR6p24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ppCount(c):\n",
        "  personPronoun = 0\n",
        "  file = open(str(url_id[c])+'.txt','r')\n",
        "  f = file.read()\n",
        "  x = re.split('[ |\\n]',f)\n",
        "  for i in x:\n",
        "    if i.upper() in ['I','WE','US''MY','OURS']:\n",
        "          personPronoun += 1\n",
        "  return personPronoun"
      ],
      "metadata": {
        "id": "tuWWjhZN7xiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def countingFunction(lst,c):\n",
        "  words = 0\n",
        "  sentences = 0\n",
        "  complexWords = 0\n",
        "  syllablePerWord = 0\n",
        "  personPronoun = 0\n",
        "  wordLength = 0\n",
        "  for i in lst:\n",
        "    if i in ['.',' . ',' .','. ']:\n",
        "\n",
        "      sentences += 1\n",
        "    elif i not in ['.',' . ',' .','. ']:\n",
        "      words += 1\n",
        "      syllable_count = 0\n",
        "      vowels = 'aeiouy'\n",
        "      wordLength += len(i)\n",
        "      if i[0] in vowels:\n",
        "          syllable_count += 1\n",
        "      l = len(i)\n",
        "      for index in range(1, l):\n",
        "          if i[index] in vowels and i[index - 1] not in vowels:\n",
        "              syllable_count += 1\n",
        "      if i.endswith('e'):\n",
        "          syllable_count -= 1\n",
        "      if i.endswith('le') and l > 2 and i[-3] not in vowels:\n",
        "          syllable_count += 1\n",
        "      if syllable_count == 0:\n",
        "          syllable_count += 1\n",
        "      syllablePerWord += syllable_count\n",
        "      if syllable_count > 2:\n",
        "        complexWords += 1\n",
        "      personPronoun = ppCount(c)\n",
        "\n",
        "  return words, sentences, complexWords, syllablePerWord, personPronoun, wordLength\n"
      ],
      "metadata": {
        "id": "mcw_nDuSc5dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avgSenLenLst = []\n",
        "perComplexWordsLst = []\n",
        "fogIndexLst = []\n",
        "avgNoWordPerSenLst = []\n",
        "complexWordCountLst = []\n",
        "wordCountLst = []\n",
        "syllablePerWordLst = []\n",
        "personalPronounLst = []\n",
        "avgWordLenLst = []\n",
        "length = len(allFiles)\n",
        "for i in range(length):\n",
        "  words, sentences, complexWords, syllablePerWord, personPronoun, wordLength = countingFunction(allFiles[i],i)\n",
        "  if sentences == 0 or words == 0:\n",
        "    avgSenLenLst.append(0)\n",
        "    perComplexWordsLst.append(0)\n",
        "    fogIndexLst.append(0)\n",
        "    avgNoWordPerSenLst.append(0)\n",
        "    complexWordCountLst.append(0)\n",
        "    wordCountLst.append(0)\n",
        "    syllablePerWordLst.append(0)\n",
        "    personalPronounLst.append(0)\n",
        "    avgWordLenLst.append(0)\n",
        "\n",
        "  else:\n",
        "    avgSenLenLst.append(words/sentences)\n",
        "    perComplexWordsLst.append(complexWords/words)\n",
        "    fogIndexLst.append(0.4*(avgSenLenLst[i] + perComplexWordsLst[i]))\n",
        "    avgNoWordPerSenLst.append(words/sentences)\n",
        "    complexWordCountLst.append(complexWords)\n",
        "    wordCountLst.append(words)\n",
        "    syllablePerWordLst.append(syllablePerWord)\n",
        "    personalPronounLst.append(personPronoun)\n",
        "    avgWordLenLst.append(wordLength/words)\n",
        "\n"
      ],
      "metadata": {
        "id": "MgAv-XsflRbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cell_value = 'AVG SENTENCE LENGTH'\n",
        "row_num, column_num = find_cell_coordinates(worksheet, cell_value)\n",
        "for r in range(1,num_of_urls+1):\n",
        "  worksheet.cell(row=row_num+r, column=column_num).value = avgSenLenLst[r-1]\n",
        "  worksheet.cell(row=row_num+r, column=column_num+1).value = perComplexWordsLst[r-1]\n",
        "  worksheet.cell(row=row_num+r, column=column_num+2).value = fogIndexLst[r-1]\n",
        "  worksheet.cell(row=row_num+r, column=column_num+3).value = avgNoWordPerSenLst[r-1]\n",
        "  worksheet.cell(row=row_num+r, column=column_num+4).value = complexWordCountLst[r-1]\n",
        "  worksheet.cell(row=row_num+r, column=column_num+5).value = wordCountLst[r-1]\n",
        "  worksheet.cell(row=row_num+r, column=column_num+6).value = syllablePerWordLst[r-1]\n",
        "  worksheet.cell(row=row_num+r, column=column_num+7).value = personalPronounLst[r-1]\n",
        "  worksheet.cell(row=row_num+r, column=column_num+8).value = avgWordLenLst[r-1]\n"
      ],
      "metadata": {
        "id": "ySUI08Li1SIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "workbook.save(filename='Output Data Structure.xlsx')"
      ],
      "metadata": {
        "id": "5BgJh8u13459"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mksaoMe-6zSm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}